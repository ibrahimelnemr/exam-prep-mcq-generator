{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 0 - GET GOOGLE RESULTS PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# File to store all results\n",
    "filename = 'examtopics_urls.txt'\n",
    "filename = 'google_results.txt'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Open the file for writing results\n",
    "with open(filename, 'w') as file:\n",
    "    for question_number in range(1, 284):  # Loop from 1 to 283\n",
    "        search_topic = f\"Exam Associate Cloud Engineer question {question_number}\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_topic.replace(' ', '+')}\"\n",
    "        \n",
    "        attempt = 0\n",
    "        max_attempts = 3\n",
    "        first_url = None\n",
    "        \n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                # Send the request\n",
    "                response = requests.get(search_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                file.write(soup.text)\n",
    "                \n",
    "                # Find the first search result URL\n",
    "                # result = soup.find('div', class_='tF2Cxc')\n",
    "                title_element = soup.select_one('h3.LC20lb.MBeuO.DKV0Md')\n",
    "                print(title_element.text if title_element else \"No title found\")\n",
    "\n",
    "                result = soup.select_one('div.zReHs a')\n",
    "                first_url = result.find('a')['href'] if result else None\n",
    "                \n",
    "\n",
    "                if first_url:\n",
    "                    print(f\"Question {question_number}: {first_url}\")\n",
    "                    file.write(f\"Question {question_number}: {first_url}\\n\")\n",
    "                    break  # Exit retry loop if URL is found\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt + 1}: No valid URL found for question {question_number}. Retrying...\")\n",
    "                    time.sleep(1)  # Wait 1 second before retrying\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: An error occurred for question {question_number}: {e}\")\n",
    "            \n",
    "            attempt += 1\n",
    "            break\n",
    "\n",
    "        # If no URL was found after max attempts, log failure\n",
    "        if not first_url:\n",
    "            print(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\")\n",
    "            file.write(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\\n\")\n",
    "        \n",
    "        # Wait 1 second between searches to avoid rate-limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"\\nSearch completed. All results saved in:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - GET LINK URLS FROM GOOGLE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# File to store all results\n",
    "filename = 'examtopics_urls.txt'\n",
    "filename = 'google_results.txt'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Open the file for writing results\n",
    "with open(filename, 'w') as file:\n",
    "    for question_number in range(1, 284):  # Loop from 1 to 283\n",
    "        search_topic = f\"Exam Associate Cloud Engineer question {question_number}\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_topic.replace(' ', '+')}\"\n",
    "        \n",
    "        attempt = 0\n",
    "        max_attempts = 3\n",
    "        first_url = None\n",
    "        \n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                # Send the request\n",
    "                response = requests.get(search_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                file.write(soup.text)\n",
    "                \n",
    "                # Find the first search result URL\n",
    "                # result = soup.find('div', class_='tF2Cxc')\n",
    "                title_element = soup.select_one('h3.LC20lb.MBeuO.DKV0Md')\n",
    "                print(title_element.text if title_element else \"No title found\")\n",
    "\n",
    "                result = soup.select_one('div.zReHs a')\n",
    "                first_url = result.find('a')['href'] if result else None\n",
    "                \n",
    "\n",
    "                if first_url:\n",
    "                    print(f\"Question {question_number}: {first_url}\")\n",
    "                    file.write(f\"Question {question_number}: {first_url}\\n\")\n",
    "                    break  # Exit retry loop if URL is found\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt + 1}: No valid URL found for question {question_number}. Retrying...\")\n",
    "                    time.sleep(1)  # Wait 1 second before retrying\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: An error occurred for question {question_number}: {e}\")\n",
    "            \n",
    "            attempt += 1\n",
    "            break\n",
    "\n",
    "        # If no URL was found after max attempts, log failure\n",
    "        if not first_url:\n",
    "            print(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\")\n",
    "            file.write(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\\n\")\n",
    "        \n",
    "        # Wait 1 second between searches to avoid rate-limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"\\nSearch completed. All results saved in:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - Download HTML FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure the directory exists\n",
    "save_dir = \"examtopics-html-files\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.examtopics.com/discussions/google/view/21681-exam-associate-cloud-engineer-topic-1-question-1-discussion/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def clean_filename(title):\n",
    "    invalid_chars = '<>:\"/\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        title = title.replace(char, '_')\n",
    "    return title.strip()\n",
    "\n",
    "\n",
    "# Fetch page\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    page_title = soup.title.text.strip() if soup.title else \"ExamTopics\"\n",
    "\n",
    "    # Format the filename\n",
    "    filename = page_title.replace(\":\", \" -\").replace(\"/\", \"-\") + \".html\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "    # Save HTML\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "\n",
    "    print(f\"✅ Downloaded: {filename}\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Failed to download {url}, Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set the download path\n",
    "download_path = os.curdir + '/res'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "\n",
    "# Function to clean the file name\n",
    "def clean_filename(title):\n",
    "    invalid_chars = '<>:\"/\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        title = title.replace(char, '_')\n",
    "    return title.strip()\n",
    "\n",
    "# Function to download and save the webpage\n",
    "def download_webpage(url, folder):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        # Parse the HTML content to extract the title\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            title = title_tag.text\n",
    "            filename = clean_filename(title) + '.html'\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(os.path.join(folder, filename)):\n",
    "                i = 1\n",
    "                while os.path.exists(os.path.join(folder, f\"{filename[:-5]}_{i}.html\")):\n",
    "                    i += 1\n",
    "                filename = f\"{filename[:-5]}_{i}.html\"\n",
    "            # Save the webpage content\n",
    "            with open(os.path.join(folder, filename), 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"No title found for URL: {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# Read the list of URLs from the file\n",
    "with open('Exam_Associate_Cloud_Engineer_Results.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract URLs from lines\n",
    "urls = [line.split(': ')[1].strip() for line in lines if line.startswith('Question')]\n",
    "\n",
    "# Download each webpage with a delay\n",
    "for url in urls:\n",
    "    download_webpage(url, download_path)\n",
    "    time.sleep(5)  # Delay in seconds to avoid getting blocked\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 - EXTRACT DATA FROM DOWNLOADED HTML FILES AND SAVE TO JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Use system-installed Chromedriver\n",
    "service = Service(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "\n",
    "# Configure Chromium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"/usr/bin/chromium-browser\"\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Mimic a real browser (Update User-Agent)\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Disable WebDriver detection\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Perform Google search\n",
    "query = \"Exam Associate Cloud Engineer question 1\"\n",
    "search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
    "\n",
    "driver.get(search_url)\n",
    "time.sleep(random.uniform(3, 7))  # Random delay to simulate human browsing\n",
    "\n",
    "# Scroll a bit to look more human\n",
    "driver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Extract first search result\n",
    "try:\n",
    "    first_result = driver.find_element(By.CSS_SELECTOR, \"h3\")\n",
    "    print(\"First result title:\", first_result.text)\n",
    "except:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.examprepper.co/exam/1\"\n",
    "num_pages = 57\n",
    "questions_data = []\n",
    "\n",
    "for i in range(1, num_pages + 1):\n",
    "    url = f\"{base_url}/{i}\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raises an error for HTTP 4xx/5xx status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch page {i} - {url}\")\n",
    "        print(f\"HTTP Status: {response.status_code if 'response' in locals() else 'No Response'}\")\n",
    "        print(f\"Response Body:\\n{response.text if 'response' in locals() else 'No Response Body'}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract question number\n",
    "    question_number_tag = soup.select_one(\"div.css-1t965vy:nth-of-type(3)\")\n",
    "    question_number = question_number_tag.text.strip() if question_number_tag else f\"Unknown {i}\"\n",
    "\n",
    "    # Extract question text\n",
    "    question_text_tag = soup.select_one(\"div.css-naa3lg p.css-oscyi5\")\n",
    "    question_text = question_text_tag.text.strip() if question_text_tag else \"No question text found\"\n",
    "\n",
    "    # Extract MCQ answer options\n",
    "    answers = []\n",
    "    answer_options = soup.select(\"div.css-1t965vy.css-j7qwjs div.css-1hd35cf\")\n",
    "\n",
    "    for option in answer_options:\n",
    "        letter_tag = option.select_one(\"div.css-1fdcwt3 p.css-xakj1w\")\n",
    "        text_tag = option.select_one(\"div.css-cba290 p.chakra-text.css-0\")\n",
    "        \n",
    "        letter = letter_tag.text.strip() if letter_tag else \"?\"\n",
    "        text = text_tag.text.strip() if text_tag else \"No answer text\"\n",
    "\n",
    "        answers.append({\"letter\": letter, \"text\": text})\n",
    "\n",
    "    # Store extracted data\n",
    "    question_entry = {\n",
    "        \"question_number\": question_number,\n",
    "        \"question_text\": question_text,\n",
    "        \"answers\": answers\n",
    "    }\n",
    "    questions_data.append(question_entry)\n",
    "    print(question_entry)\n",
    "\n",
    "# Save data as JSON\n",
    "with open(\"questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(questions_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Scraping completed and saved to questions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.examprepper.co/exam/1\" # associate cloud engineer\n",
    "\n",
    "num_pages = 57\n",
    "\n",
    "for i in range(1, num_pages + 1):\n",
    "    url = f\"{base_url}/{i}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(soup)\n",
    "\n",
    "# div class css-naa3lg\n",
    "    # p class css-oscyi5\n",
    "    # contains question text\n",
    "\n",
    "# div class css-1t965vy\n",
    "    # 3rd child is question number\n",
    "\n",
    "\n",
    "# div class css-1t965vy css-j7qwjs list of mcq answer options\n",
    "    # css-1hd35cf question box\n",
    "        # div css-1fdcwt3\n",
    "            # p css-xakj1w\n",
    "            # answer option letter\n",
    "        # div css-cba290\n",
    "            # p chakra-text css-0\n",
    "            # answer option text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-exam-prep)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
