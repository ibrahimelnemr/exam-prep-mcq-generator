{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Use system-installed Chromedriver\n",
    "service = Service(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "\n",
    "# Configure Chromium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"/usr/bin/chromium-browser\"\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Mimic a real browser (Update User-Agent)\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Disable WebDriver detection\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Perform Google search\n",
    "query = \"Exam Associate Cloud Engineer question 1\"\n",
    "search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
    "\n",
    "driver.get(search_url)\n",
    "time.sleep(random.uniform(3, 7))  # Random delay to simulate human browsing\n",
    "\n",
    "# Scroll a bit to look more human\n",
    "driver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Extract first search result\n",
    "try:\n",
    "    first_result = driver.find_element(By.CSS_SELECTOR, \"h3\")\n",
    "    print(\"First result title:\", first_result.text)\n",
    "except:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.examprepper.co/exam/1\"\n",
    "num_pages = 57\n",
    "questions_data = []\n",
    "\n",
    "for i in range(1, num_pages + 1):\n",
    "    url = f\"{base_url}/{i}\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raises an error for HTTP 4xx/5xx status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch page {i} - {url}\")\n",
    "        print(f\"HTTP Status: {response.status_code if 'response' in locals() else 'No Response'}\")\n",
    "        print(f\"Response Body:\\n{response.text if 'response' in locals() else 'No Response Body'}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract question number\n",
    "    question_number_tag = soup.select_one(\"div.css-1t965vy:nth-of-type(3)\")\n",
    "    question_number = question_number_tag.text.strip() if question_number_tag else f\"Unknown {i}\"\n",
    "\n",
    "    # Extract question text\n",
    "    question_text_tag = soup.select_one(\"div.css-naa3lg p.css-oscyi5\")\n",
    "    question_text = question_text_tag.text.strip() if question_text_tag else \"No question text found\"\n",
    "\n",
    "    # Extract MCQ answer options\n",
    "    answers = []\n",
    "    answer_options = soup.select(\"div.css-1t965vy.css-j7qwjs div.css-1hd35cf\")\n",
    "\n",
    "    for option in answer_options:\n",
    "        letter_tag = option.select_one(\"div.css-1fdcwt3 p.css-xakj1w\")\n",
    "        text_tag = option.select_one(\"div.css-cba290 p.chakra-text.css-0\")\n",
    "        \n",
    "        letter = letter_tag.text.strip() if letter_tag else \"?\"\n",
    "        text = text_tag.text.strip() if text_tag else \"No answer text\"\n",
    "\n",
    "        answers.append({\"letter\": letter, \"text\": text})\n",
    "\n",
    "    # Store extracted data\n",
    "    question_entry = {\n",
    "        \"question_number\": question_number,\n",
    "        \"question_text\": question_text,\n",
    "        \"answers\": answers\n",
    "    }\n",
    "    questions_data.append(question_entry)\n",
    "    print(question_entry)\n",
    "\n",
    "# Save data as JSON\n",
    "with open(\"questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(questions_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Scraping completed and saved to questions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.examprepper.co/exam/1\" # associate cloud engineer\n",
    "\n",
    "num_pages = 57\n",
    "\n",
    "for i in range(1, num_pages + 1):\n",
    "    url = f\"{base_url}/{i}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(soup)\n",
    "\n",
    "# div class css-naa3lg\n",
    "    # p class css-oscyi5\n",
    "    # contains question text\n",
    "\n",
    "# div class css-1t965vy\n",
    "    # 3rd child is question number\n",
    "\n",
    "\n",
    "# div class css-1t965vy css-j7qwjs list of mcq answer options\n",
    "    # css-1hd35cf question box\n",
    "        # div css-1fdcwt3\n",
    "            # p css-xakj1w\n",
    "            # answer option letter\n",
    "        # div css-cba290\n",
    "            # p chakra-text css-0\n",
    "            # answer option text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"card-text\">\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\tEvery employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?<br/>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# url = \"https://www.examprepper.co/exam/1/1\"\n",
    "url = \"https://www.examtopics.com/discussions/google/view/21681-exam-associate-cloud-engineer-topic-1-question-1-discussion/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "exam_data = soup.find_all('div', class_='exam-data')\n",
    "question_text = soup.select_one(\"div.question-body p.card-text\")\n",
    "print(question_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/ipykernel_52206/104986007.py:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  invalid_chars = '<>:\"/\\|?*'\n"
     ]
    }
   ],
   "source": [
    "def clean_filename(title):\n",
    "    invalid_chars = '<>:\"/\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        title = title.replace(char, '_')\n",
    "    return title.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get URLs from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No title found\n",
      "Attempt 1: No valid URL found for question 1. Retrying...\n",
      "No title found\n",
      "Attempt 2: No valid URL found for question 1. Retrying...\n",
      "No title found\n",
      "Attempt 3: No valid URL found for question 1. Retrying...\n",
      "Question 1: No valid URL found after 3 attempts.\n",
      "No title found\n",
      "Attempt 1: No valid URL found for question 2. Retrying...\n",
      "No title found\n",
      "Attempt 2: No valid URL found for question 2. Retrying...\n",
      "No title found\n",
      "Attempt 3: No valid URL found for question 2. Retrying...\n",
      "Question 2: No valid URL found after 3 attempts.\n",
      "No title found\n",
      "Attempt 1: No valid URL found for question 3. Retrying...\n",
      "No title found\n",
      "Attempt 2: No valid URL found for question 3. Retrying...\n",
      "No title found\n",
      "Attempt 3: No valid URL found for question 3. Retrying...\n",
      "Question 3: No valid URL found after 3 attempts.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m             file.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: No valid URL found after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# Wait 1 second between searches to avoid rate-limiting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSearch completed. All results saved in:\u001b[39m\u001b[33m\"\u001b[39m, filename)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# File to store all results\n",
    "filename = 'examtopics_urls.txt'\n",
    "filename = 'google_results.txt'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Open the file for writing results\n",
    "with open(filename, 'w') as file:\n",
    "    for question_number in range(1, 284):  # Loop from 1 to 283\n",
    "        search_topic = f\"Exam Associate Cloud Engineer question {question_number}\"\n",
    "        search_url = f\"https://www.google.com/search?q={search_topic.replace(' ', '+')}\"\n",
    "        \n",
    "        attempt = 0\n",
    "        max_attempts = 3\n",
    "        first_url = None\n",
    "        \n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                # Send the request\n",
    "                response = requests.get(search_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                file.write(soup.text)\n",
    "                \n",
    "                # Find the first search result URL\n",
    "                # result = soup.find('div', class_='tF2Cxc')\n",
    "                title_element = soup.select_one('h3.LC20lb.MBeuO.DKV0Md')\n",
    "                print(title_element.text if title_element else \"No title found\")\n",
    "\n",
    "                result = soup.select_one('div.zReHs a')\n",
    "                first_url = result.find('a')['href'] if result else None\n",
    "                \n",
    "\n",
    "                if first_url:\n",
    "                    print(f\"Question {question_number}: {first_url}\")\n",
    "                    file.write(f\"Question {question_number}: {first_url}\\n\")\n",
    "                    break  # Exit retry loop if URL is found\n",
    "                else:\n",
    "                    print(f\"Attempt {attempt + 1}: No valid URL found for question {question_number}. Retrying...\")\n",
    "                    time.sleep(1)  # Wait 1 second before retrying\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: An error occurred for question {question_number}: {e}\")\n",
    "            \n",
    "            attempt += 1\n",
    "            break\n",
    "\n",
    "        # If no URL was found after max attempts, log failure\n",
    "        if not first_url:\n",
    "            print(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\")\n",
    "            file.write(f\"Question {question_number}: No valid URL found after {max_attempts} attempts.\\n\")\n",
    "        \n",
    "        # Wait 1 second between searches to avoid rate-limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"\\nSearch completed. All results saved in:\", filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/ipykernel_52206/2303734113.py:15: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  invalid_chars = '<>:\"/\\|?*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 1 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 2 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 3 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 4 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 5 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 6 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 7 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 8 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 9 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 10 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 11 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 12 discussion - ExamTopics.html\n",
      "Downloaded: Exam Associate Cloud Engineer topic 1 question 13 discussion - ExamTopics.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[32m     58\u001b[39m     download_webpage(url, download_path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Delay in seconds to avoid getting blocked\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDownload complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set the download path\n",
    "download_path = os.curdir + '/res'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "\n",
    "# Function to clean the file name\n",
    "def clean_filename(title):\n",
    "    invalid_chars = '<>:\"/\\|?*'\n",
    "    for char in invalid_chars:\n",
    "        title = title.replace(char, '_')\n",
    "    return title.strip()\n",
    "\n",
    "# Function to download and save the webpage\n",
    "def download_webpage(url, folder):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        # Parse the HTML content to extract the title\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            title = title_tag.text\n",
    "            filename = clean_filename(title) + '.html'\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(os.path.join(folder, filename)):\n",
    "                i = 1\n",
    "                while os.path.exists(os.path.join(folder, f\"{filename[:-5]}_{i}.html\")):\n",
    "                    i += 1\n",
    "                filename = f\"{filename[:-5]}_{i}.html\"\n",
    "            # Save the webpage content\n",
    "            with open(os.path.join(folder, filename), 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"No title found for URL: {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# Read the list of URLs from the file\n",
    "with open('Exam_Associate_Cloud_Engineer_Results.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract URLs from lines\n",
    "urls = [line.split(': ')[1].strip() for line in lines if line.startswith('Question')]\n",
    "\n",
    "# Download each webpage with a delay\n",
    "for url in urls:\n",
    "    download_webpage(url, download_path)\n",
    "    time.sleep(5)  # Delay in seconds to avoid getting blocked\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-exam-prep)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
